{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVnUSCIR49ON"
      },
      "source": [
        "# FOSSistant difficulty prediction model v0.3.0 data processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe19hHN95MkV"
      },
      "source": [
        "## Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bn959RTukRCo"
      },
      "outputs": [],
      "source": [
        "%pip install -U -q datasets bitsandbytes \"huggingface_hub[hf_xet]\" \"huggingface_hub[hf_transfer]\" \"distilabel[hf-transformers]\" outlines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYRjODFGoZnD"
      },
      "outputs": [],
      "source": [
        "# GDRIVE_DIR = r\"/content/drive/\"\n",
        "ROOT_DIR = r\"~/\"\n",
        "\n",
        "MODEL_PATH = \"answerdotai/ModernBERT-large\"\n",
        "# MODEL_PATH = r\"answerdotai/ModernBERT-base\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbOWkJffXK8N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "!export HF_HUB_ENABLE_HF_TRANSFER=1\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "torch.set_float32_matmul_precision(\"high\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9w-AxSA1Oft"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import base64\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import pipeline, BitsAndBytesConfig\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "from datasets import (Dataset, DatasetDict,\n",
        "                      Features, Value, ClassLabel,\n",
        "                      load_dataset, load_from_disk,\n",
        "                      concatenate_datasets)\n",
        "from huggingface_hub import login\n",
        "\n",
        "# import kagglehub\n",
        "# from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "from pydantic import BaseModel\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "from cerebras.cloud.sdk import Cerebras\n",
        "from mistralai import Mistral\n",
        "\n",
        "# import distilabel\n",
        "# from distilabel.pipeline import Pipeline\n",
        "# from distilabel.models import TransformersLLM\n",
        "# from distilabel.steps import LoadDataFromFileSystem\n",
        "# from distilabel.steps.tasks import TextClassification\n",
        "\n",
        "# login(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdOcRp_D_tpC"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount(GDRIVE_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zs0OG3v5aRT"
      },
      "source": [
        "## Initial dataset preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-RmbIRAt_fQ"
      },
      "outputs": [],
      "source": [
        "!mkdir -p $ROOT_DIR/datasets/fossistant/\n",
        "!wget https://figshare.com/ndownloader/files/35739797 -O $ROOT_DIR/datasets/fossistant/github_issues_figshare_original.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AeVRFqoDyVO5"
      },
      "outputs": [],
      "source": [
        "ds_1 = load_dataset(\"mlfoundations-dev/github-issues\")\n",
        "ds_1 = concatenate_datasets([ds_1[\"train\"], ds_1[\"test\"]])\n",
        "\n",
        "df_1 = ds_1.to_pandas()\n",
        "df_1 = df_1[[\"repo_name\", \"issue_number\", \"title\", \"body\", \"labels\"]]\n",
        "\n",
        "\n",
        "df_2 = pd.read_json(ROOT_DIR + \"datasets/fossistant/github_issues_figshare_original.json\")\n",
        "df_2[[\"repo_name\", \"issue_number\"]] = df_2[\"url\"].str.extract(r\"repos\\/(.+)\\/issues\\/(\\d+)\")\n",
        "df_2 = df_2[[\"repo_name\", \"issue_number\", \"title\", \"body\", \"labels\"]]\n",
        "\n",
        "\n",
        "# ds_3 = load_dataset(\"bigcode/the-stack-github-issues\")\n",
        "\n",
        "# df_3 = ds_3.to_pandas()\n",
        "# df_3 = df_3[[\"title\", \"body\"]]\n",
        "\n",
        "\n",
        "df = pd.concat([df_1, df_2], ignore_index=True)\n",
        "# df = pd.concat([df_1, df_2, df_3], ignore_index=True)\n",
        "df[\"issue_number\"] = df[\"issue_number\"].astype(int)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDhLyCdvFH4n"
      },
      "outputs": [],
      "source": [
        "df[\"repo_name_lower\"] = df[\"repo_name\"].str.lower()\n",
        "subset = [\"repo_name_lower\", \"issue_number\"]\n",
        "\n",
        "display(df.duplicated(subset=subset).value_counts())\n",
        "df = df.drop_duplicates(subset=subset, ignore_index=True)\n",
        "display(df.duplicated(subset=subset).value_counts())\n",
        "df = df.drop(columns=[\"repo_name_lower\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eil1Xi41axxf"
      },
      "outputs": [],
      "source": [
        "df[\"repo_name\"].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVrzVkdpaaVq"
      },
      "outputs": [],
      "source": [
        "display(df.isnull().sum())\n",
        "df[\"body\"] = df[\"body\"].fillna(\"\")\n",
        "# df = df.dropna(ignore_index=True)\n",
        "display(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVEX6Pf5CRF2"
      },
      "source": [
        "## Rule-based dataset annotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DAgLgnY9lHhh"
      },
      "outputs": [],
      "source": [
        "pattern = re.compile(r\"easy|newbie|begin|starter|started|^minor$|bug.*minor|minor.*fix|p-minor|novice|grab|good.*first|first.*time|low.*fruit|small$|^low$|effort.*low|estimate.*low|task.*low|level.*low|difficulty.*low\", re.IGNORECASE)\n",
        "easy_loc = df[\"labels\"].apply(lambda labels: any(pattern.search(label) for label in labels))\n",
        "\n",
        "easy_df = df[easy_loc]\n",
        "easy_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wa9j66qD2A1Z"
      },
      "outputs": [],
      "source": [
        "pattern = re.compile(r\"intermediate|good.*second|effort.*medium|estimate.*medium|task.*medium|level.*medium|difficulty.*medium\", re.IGNORECASE)\n",
        "medium_loc = df[\"labels\"].apply(lambda labels: any(pattern.search(label) for label in labels))\n",
        "\n",
        "medium_df = df[medium_loc & ~easy_loc]\n",
        "medium_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYxUr7NE2Frb"
      },
      "outputs": [],
      "source": [
        "# pattern = re.compile(r\"important|major|breaking|hard|serious|advanced|large|^long$|effort.*long|long.*term|p0|p1|critical|difficult$|^core$|^expert$|effort.*expert|estimate.*expert|task.*expert|level.*expert|difficulty.*expert\", re.IGNORECASE)\n",
        "pattern = re.compile(r\"hard|serious|advanced|large|difficult$|^expert$|effort.*expert|estimate.*expert|task.*expert|level.*expert|difficulty.*expert\", re.IGNORECASE)\n",
        "hard_loc = df[\"labels\"].apply(lambda labels: any(pattern.search(label) for label in labels))\n",
        "\n",
        "hard_df = df[hard_loc & ~easy_loc & ~medium_loc]\n",
        "hard_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tk9BOehA2prs"
      },
      "outputs": [],
      "source": [
        "# pattern = re.compile(r\"need.*more|request.*comment\", re.IGNORECASE)\n",
        "# misc_loc = df[\"labels\"].apply(lambda labels: any(pattern.search(label) for label in labels))\n",
        "\n",
        "# misc_df = df[misc_loc & ~easy_loc & ~medium_loc & ~hard_loc]\n",
        "# misc_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArYkPfU16VMo"
      },
      "outputs": [],
      "source": [
        "df[\"labels\"].apply(lambda labels: [label for label in labels if re.compile(r\"request.*comment\", re.IGNORECASE).search(label)] or None).value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__bYR59jirp3"
      },
      "outputs": [],
      "source": [
        "with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None, \"display.max_colwidth\", None):\n",
        "    display(df[df[\"labels\"].apply(lambda labels: any(re.compile(r\"request.*comment\", re.IGNORECASE).search(label) for label in labels))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWCDw01FruZv"
      },
      "outputs": [],
      "source": [
        "# rest_df = df[~easy_loc & ~medium_loc & ~hard_loc & ~misc_loc]\n",
        "rest_df = df[~easy_loc & ~medium_loc & ~hard_loc]\n",
        "rest_df = rest_df.reset_index(drop=True)\n",
        "rest_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVrd2XU79U8M"
      },
      "outputs": [],
      "source": [
        "easy_df = easy_df.assign(difficulty=0, difficulty_model=\"rule\")\n",
        "medium_df = medium_df.assign(difficulty=1, difficulty_model=\"rule\")\n",
        "hard_df = hard_df.assign(difficulty=2, difficulty_model=\"rule\")\n",
        "# misc_df = misc_df.assign(difficulty=3, difficulty_model=\"rule\")\n",
        "\n",
        "rest_df = rest_df.assign(difficulty=4, difficulty_model=\"none\")\n",
        "rest_df = rest_df.sample(frac=1, random_state=42, ignore_index=True) # Not classified (unknown)\n",
        "\n",
        "# merged_df = pd.concat([easy_df, medium_df, hard_df, misc_df, rest_df], ignore_index=True)\n",
        "merged_df = pd.concat([easy_df, medium_df, hard_df, rest_df], ignore_index=True)\n",
        "merged_df.to_json(ROOT_DIR + \"datasets/fossistant/github_issues_rule_based_annotation.jsonl\",\n",
        "                  orient=\"records\", lines=True)\n",
        "merged_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyGAjWe0Chhk"
      },
      "source": [
        "## LLM-based dataset annotation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jj99Lmat8Le"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT = '# Instruction\\nPlease classify the github issue by assigning the most appropriate labels.\\nDo not explain your reasoning or provide any additional commentary.\\nIf the text is ambiguous or lacks sufficient information for classification, respond with \"unknown\".\\nProvide the label that best describes the text.\\nDetermine the difficulty of the GitHub issue.\\n\\n## Labeling the user input\\nUse the available labels to classify the user query. Analyze the context of each label specifically:\\navailable_labels = [\\n    \"easy\",  # A beginner-friendly issue that doesn\\'t require much prior experience.\\n    \"medium\",  # An issue suitable for contributors with intermediate-level skills and experience.\\n    \"hard\",  # A challenging issue that likely requires advanced or specialized expertise.\\n    \"misc\",  # An issue that doesn\\'t involve direct problem-solving, such as general discussions or non-technical topics, so its difficulty can\\'t be assessed.\\n    \"unknown\",  # An issue with unclear requirements or scope, making its difficulty hard to determine.\\n]\\n\\n## Examples\\n### Input\\n```\\nTitle: A small typo in docs\\nBody: I found a small typo in docs!\\nLabels: [\\'good first issue\\', \\'docs\\']\\n```\\n### Output\\n```\\neasy\\n```\\n\\n## Output Format\\nNow, please give me the labels in as-is raw text format, do not include any other text in your response:\\n```\\nlabel\\n```'\n",
        "print(SYSTEM_PROMPT)\n",
        "print(\"\\n\\n---\\n\\n\")\n",
        "\n",
        "USER_TEMPLATE = 'Title: {title}\\nBody: {body}\\nLabels: {labels}'\n",
        "print(USER_TEMPLATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGyP28zdncIx"
      },
      "outputs": [],
      "source": [
        "df = pd.read_json(ROOT_DIR + \"datasets/fossistant/github_issues_rule_based_annotation.jsonl\",\n",
        "                  lines=True)\n",
        "\n",
        "df[\"messages\"] = df[[\"title\", \"body\", \"labels\"]].apply(\n",
        "    lambda row: [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": SYSTEM_PROMPT,\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": USER_TEMPLATE.format(title=row[\"title\"], body=row[\"body\"], labels=row[\"labels\"]),\n",
        "        },\n",
        "    ],\n",
        "    axis=1,\n",
        ")\n",
        "df = df[[\"messages\", \"difficulty\", \"difficulty_model\"]]\n",
        "\n",
        "df.to_json(ROOT_DIR + \"datasets/fossistant/github_issues_llm_based_annotation_input.jsonl\",\n",
        "           orient=\"records\", lines=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FOAPgeRncIx"
      },
      "outputs": [],
      "source": [
        "df = pd.read_json(ROOT_DIR + \"datasets/fossistant/github_issues_llm_based_annotation_input.jsonl\",\n",
        "                  lines=True)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ALiu6jtncIy"
      },
      "outputs": [],
      "source": [
        "client = OpenAI(\n",
        "    # api_key=\"\",\n",
        "    # base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
        "\n",
        "    api_key=\"\",\n",
        "    base_url=\"https://api.cerebras.ai/v1\",\n",
        ")\n",
        "\n",
        "models = [\n",
        "    # \"gemini-2.5-flash-preview-05-20\",\n",
        "    # \"gemini-2.0-flash\",\n",
        "    # \"gemini-2.0-flash-lite\",\n",
        "    # \"gemini-1.5-flash\",\n",
        "\n",
        "    \"llama-4-scout-17b-16e-instruct\",\n",
        "]\n",
        "model_index = 0\n",
        "\n",
        "class Difficulty(BaseModel):\n",
        "    difficulty: str\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    if row[\"difficulty_model\"] != \"none\":\n",
        "        continue\n",
        "\n",
        "    while True:\n",
        "        model = models[model_index]\n",
        "\n",
        "        try:\n",
        "            completion = client.beta.chat.completions.parse(\n",
        "                model=model,\n",
        "                messages=row[\"messages\"],\n",
        "                response_format=Difficulty,\n",
        "            )\n",
        "\n",
        "            difficulty = completion.choices[0].message.parsed.difficulty.strip().lower()\n",
        "            if \"easy\" in difficulty:\n",
        "                difficulty_index = 0\n",
        "            elif \"medium\" in difficulty:\n",
        "                difficulty_index = 1\n",
        "            elif \"hard\" in difficulty:\n",
        "                difficulty_index = 2\n",
        "            elif \"misc\" in difficulty:\n",
        "                difficulty_index = 3\n",
        "            else:\n",
        "                difficulty_index = 4\n",
        "\n",
        "            df.loc[index, \"difficulty_model\"] = model\n",
        "            df.loc[index, \"difficulty\"] = difficulty_index\n",
        "\n",
        "            break\n",
        "        except openai.RateLimitError:\n",
        "            print(f\"RateLimitError: {model} / Index: {index}\")\n",
        "\n",
        "            model_index = (model_index + 1) % len(models)\n",
        "        # except openai.BadRequestError:\n",
        "        #     print(f\"BadRequestError: {model} / Index: {index}\")\n",
        "\n",
        "        #     df.loc[index, \"difficulty_model\"] = model\n",
        "        #     df.loc[index, \"difficulty\"] = 5 # Error\n",
        "\n",
        "        #     break\n",
        "\n",
        "    if index % 100 == 0:\n",
        "        df.to_json(ROOT_DIR + \"datasets/fossistant/github_issues_llm_based_annotation_input.jsonl\",\n",
        "                   orient=\"records\", lines=True)\n",
        "        print(f\"Saved a checkpoint / Index: {index}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUAvOJE3ncIy"
      },
      "outputs": [],
      "source": [
        "df.to_json(ROOT_DIR + \"datasets/fossistant/github_issues_llm_based_annotation_input.jsonl\",\n",
        "           orient=\"records\", lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZXa4CdoncIy"
      },
      "outputs": [],
      "source": [
        "train_df = df.query(\"difficulty <= 3\")\n",
        "\n",
        "def to_train_input(messages: list):\n",
        "    content = messages[1][\"content\"]\n",
        "    labels_idx = content.rfind(\"\\nLabels:\")\n",
        "    return content[:labels_idx]\n",
        "\n",
        "train_df[\"text\"] = train_df[\"messages\"].apply(to_train_input)\n",
        "train_df = train_df[[\"text\", \"difficulty\"]]\n",
        "train_df = train_df.rename(columns={\"difficulty\": \"labels\"})\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "\n",
        "train_df.to_json(ROOT_DIR + \"datasets/fossistant/github_issues_llm_based_annotation.jsonl\",\n",
        "                 orient=\"records\", lines=True)\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrqPeol_ncIy"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_json(ROOT_DIR + \"datasets/fossistant/github_issues_llm_based_annotation.jsonl\",\n",
        "                        lines=True)\n",
        "train_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hqPB2j077ZT"
      },
      "outputs": [],
      "source": [
        "ds = Dataset.from_pandas(train_df, preserve_index=False, features=Features({\n",
        "    \"text\": Value(\"string\"),\n",
        "    \"labels\": ClassLabel(names=[\"easy\", \"medium\", \"hard\", \"misc\"]),\n",
        "}))\n",
        "\n",
        "# ds = ds.train_test_split(\n",
        "#     test_size=0.1,\n",
        "#     stratify_by_column=\"labels\",\n",
        "#     seed=42,\n",
        "# )\n",
        "\n",
        "train_valid_test_ds = ds.train_test_split(\n",
        "    test_size=0.2,\n",
        "    stratify_by_column=\"labels\",\n",
        "    seed=42,\n",
        ")\n",
        "valid_test_ds = train_valid_test_ds[\"test\"].train_test_split(\n",
        "    test_size=0.5,\n",
        "    stratify_by_column=\"labels\",\n",
        "    seed=42,\n",
        ")\n",
        "ds = DatasetDict({\n",
        "    \"train\": train_valid_test_ds[\"train\"],\n",
        "    \"valid\": valid_test_ds[\"train\"],\n",
        "    \"test\": valid_test_ds[\"test\"],\n",
        "})\n",
        "\n",
        "ds.save_to_disk(ROOT_DIR + \"datasets/fossistant/github_issues\")\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U61ax0W71j4F"
      },
      "outputs": [],
      "source": [
        "random.choice(ds[\"train\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fncFHpwncIz"
      },
      "outputs": [],
      "source": [
        "ds[\"train\"].to_pandas()[\"labels\"].value_counts()\n",
        "# ds[\"valid\"].to_pandas()[\"labels\"].value_counts()\n",
        "# ds[\"test\"].to_pandas()[\"labels\"].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtUZz7zdDAlY"
      },
      "source": [
        "## Post-processing & Saving"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VmTEt4lncIz"
      },
      "outputs": [],
      "source": [
        "ds = load_from_disk(ROOT_DIR + \"datasets/fossistant/github_issues\")\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxzuPSHQ1bA2"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "tokenizer.model_max_length = 1024\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True,\n",
        "                     return_tensors=\"pt\")\n",
        "\n",
        "tokenized_ds = ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n",
        "tokenized_ds.save_to_disk(ROOT_DIR + \"datasets/fossistant/github_issues_tokenized\")\n",
        "\n",
        "tokenized_ds[\"train\"].features.keys()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
