{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVnUSCIR49ON"
      },
      "source": [
        "# FOSSistant difficulty prediction model v0.3.0 training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe19hHN95MkV"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_B9QmuOh5Vnk"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HO8X-NoKbL3m"
      },
      "outputs": [],
      "source": [
        "%pip install -U -q datasets evaluate \"huggingface_hub[hf_xet]\" \"huggingface_hub[hf_transfer]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0UcKhgZbQ3j"
      },
      "outputs": [],
      "source": [
        "# GDRIVE_DIR = r\"/content/drive/\"\n",
        "ROOT_DIR = r\"/teamspace/studios/this_studio/\"\n",
        "\n",
        "OUTPUT_DIR = ROOT_DIR + \"models/FOSSistant-Difficulty-Prediction-v0.3.0\"\n",
        "\n",
        "MODEL_PATH = \"answerdotai/ModernBERT-large\"\n",
        "# MODEL_PATH = r\"answerdotai/ModernBERT-base\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbOWkJffXK8N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "!export HF_HUB_ENABLE_HF_TRANSFER=1\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "\n",
        "# set the wandb project where this run will be logged\n",
        "os.environ[\"WANDB_PROJECT\"]=\"FOSSistant\"\n",
        "\n",
        "# save your trained model checkpoint to wandb\n",
        "os.environ[\"WANDB_LOG_MODEL\"]=\"false\"\n",
        "\n",
        "# turn off watch to log faster\n",
        "os.environ[\"WANDB_WATCH\"]=\"false\"\n",
        "\n",
        "# !export WANDB_DISABLED=true\n",
        "# os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "random.seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dYRjODFGoZnD"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount(GDRIVE_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUqkbIrn5-jk"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zkdra5dmZo12"
      },
      "outputs": [],
      "source": [
        "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
        "                          Trainer, TrainingArguments)\n",
        "from datasets import load_from_disk\n",
        "import evaluate\n",
        "from huggingface_hub import login\n",
        "\n",
        "import wandb\n",
        "\n",
        "# login(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNqMpAi3BfXT"
      },
      "outputs": [],
      "source": [
        "tokenized_ds = load_from_disk(ROOT_DIR + \"datasets/fossistant/github_issues_tokenized\")\n",
        "tokenized_ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgQsf_uoqxH5"
      },
      "outputs": [],
      "source": [
        "labels = tokenized_ds[\"train\"].features[\"labels\"].names\n",
        "num_labels = len(labels)\n",
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = str(i)\n",
        "    id2label[str(i)] = label\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_PATH, num_labels=num_labels, label2id=label2id, id2label=id2label,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXsi_T3lNnZX"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    metric1 = evaluate.load(\"accuracy\")\n",
        "    metric2 = evaluate.load(\"f1\")\n",
        "    metric3 = evaluate.load(\"precision\")\n",
        "    metric4 = evaluate.load(\"recall\")\n",
        "\n",
        "    # average = \"weighted\"\n",
        "    average = \"macro\"\n",
        "    # average = \"micro\"\n",
        "\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    # predictions = np.argmax(predictions, axis=-1)\n",
        "\n",
        "    accuracy = metric1.compute(predictions=predictions,\n",
        "                               references=labels)[\"accuracy\"]\n",
        "    f1 = metric2.compute(predictions=predictions,\n",
        "                         references=labels,\n",
        "                         average=average)[\"f1\"]\n",
        "    precision = metric3.compute(predictions=predictions,\n",
        "                                references=labels,\n",
        "                                average=average)[\"precision\"]\n",
        "    recall = metric4.compute(predictions=predictions,\n",
        "                             references=labels,\n",
        "                             average=average)[\"recall\"]\n",
        "\n",
        "    return {\"accuracy\": accuracy,\n",
        "            \"f1\": f1,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUCM49-4q5CZ"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    logging_steps=100,\n",
        "    logging_strategy=\"steps\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    # save_strategy=\"no\",\n",
        "    overwrite_output_dir=True,\n",
        "\n",
        "    # report_to=\"tensorboard\",\n",
        "    # report_to=\"wandb\",\n",
        "    # push_to_hub=True,\n",
        "    # hub_strategy=\"every_save\",\n",
        "    # hub_token=HfFolder.get_token(),\n",
        "\n",
        "    per_device_train_batch_size=64,\n",
        "    per_device_eval_batch_size=64,\n",
        "    num_train_epochs=5,\n",
        "    learning_rate=5e-5,\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    optim=\"adamw_torch_fused\",\n",
        "    bf16=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_ds[\"train\"],\n",
        "    eval_dataset=tokenized_ds[\"test\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "On0mGO7Lq7HF"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oy3dVkYysOKo"
      },
      "outputs": [],
      "source": [
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "trainer.create_model_card()\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "# trainer.push_to_hub()\n",
        "\n",
        "!rm -r \"$OUTPUT_DIR\"/checkpoint*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIM_mJpnoNt6"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H7L0NC96J1o"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuBggLgXS2PP"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "from datasets import load_from_disk\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkN1rNXiRmWD"
      },
      "outputs": [],
      "source": [
        "ds = load_from_disk(ROOT_DIR + \"datasets/fossistant/github_issues\")\n",
        "ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qdeZLaoDN_y"
      },
      "outputs": [],
      "source": [
        "pipe = pipeline(\n",
        "    \"text-classification\",\n",
        "    # model=OUTPUT_DIR,\n",
        "    model=ROOT_DIR + \"models/FOSSistant-Difficulty-Prediction-v0.3.0-bak\",\n",
        "    device=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZsCIjnGLetf"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    metric1 = evaluate.load(\"accuracy\")\n",
        "    metric2 = evaluate.load(\"f1\")\n",
        "    metric3 = evaluate.load(\"precision\")\n",
        "    metric4 = evaluate.load(\"recall\")\n",
        "\n",
        "    # average = \"weighted\"\n",
        "    average = \"macro\"\n",
        "    # average = \"micro\"\n",
        "\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    accuracy = metric1.compute(predictions=predictions,\n",
        "                               references=labels)[\"accuracy\"]\n",
        "    f1 = metric2.compute(predictions=predictions,\n",
        "                         references=labels,\n",
        "                         average=average)[\"f1\"]\n",
        "    precision = metric3.compute(predictions=predictions,\n",
        "                                references=labels,\n",
        "                                average=average)[\"precision\"]\n",
        "    recall = metric4.compute(predictions=predictions,\n",
        "                             references=labels,\n",
        "                             average=average)[\"recall\"]\n",
        "\n",
        "    return {\"accuracy\": accuracy,\n",
        "            \"f1\": f1,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5g0JYLU-QhEy"
      },
      "outputs": [],
      "source": [
        "predictions = pipe(ds[\"test\"][\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y972IMnwPcDf"
      },
      "outputs": [],
      "source": [
        "label_mapping = {\"easy\": 0, \"medium\": 1, \"hard\": 2, \"misc\": 3}\n",
        "predicted_labels = [label_mapping[p[\"label\"]] for p in predictions]\n",
        "true_labels = ds[\"test\"][\"labels\"]\n",
        "metrics = compute_metrics((predicted_labels, true_labels))\n",
        "metrics"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
